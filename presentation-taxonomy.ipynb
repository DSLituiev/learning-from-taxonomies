{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"img/title-graphics.png\" alt=\"drawing\" width=100/></center> \n",
    "\n",
    "<center> <h1> Learning Labels from Taxonomies </h1> </center>\n",
    "<center> <h3> Joint CTAWG & UCSF NLP meetup </h3> </center>\n",
    "<center> 2021 / 02 / 19 </center>\n",
    "<center> <h3> </h3> </center>\n",
    "<center> <h3> Dima Lituiev, PhD </h3> </center>\n",
    "<center> <h3> Bakar Computational Health Sciences Institute, UCSF </h3> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Definitions\n",
    "\n",
    "## Taxonomy\n",
    "is the practice, science, systems, and principles of **classification** of things or concepts\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Taxonomy)\n",
    "\n",
    "- A taxonomy can be represented by a **simple tree graph**\n",
    "  - \"simple\" = there is one types of directed edges, with $X \\rightarrow Y$ representing \"$X$ is a child of $Y$\"\n",
    "\n",
    "## Ontology\n",
    "is specification of a conceptualization. That is, an ontology is a description (like a formal specification of a program) of the concepts and relationships that can exist for an agent or a community of agents.\n",
    "  ~[Tom Gruber, 1992](https://web.archive.org/web/20100716004426/http://www-ksl.stanford.edu/kst/what-is-an-ontology.html)\n",
    "\n",
    "- can be represented by an **arbitrary multigraph**,\n",
    " - \"multi\" = connections (relations) between entites can be of various types \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How is it relevant to biology and medicine?\n",
    "\n",
    "most medical and biological terms can be grouped into taxonomies or ontologies\n",
    "\n",
    " - i.e. OncoTree and ICD10 (10th revision of the International Statistical Classification of Diseases and Related Health Problems) are **taxonomies**\n",
    " - i.e. SNOMED CT and UMLS are **ontologies**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Taxonomy example: Oncotree\n",
    "<img src=\"img/oncotree.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "[http://oncotree.mskcc.org/](http://oncotree.mskcc.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Taxonomy example: ICD10\n",
    "\n",
    "<center>\n",
    "<img src=\"img/Althobaiti-2017-Fragment-of-ICD-10-ontology.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</center>\n",
    "  \n",
    "[Althobaiti, 2017](https://www.researchgate.net/publication/313547054_Comparison_of_Ontology-Based_Semantic-Similarity_Measures_in_the_Biomedical_Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1> Ontology example: UMLS </h1> </center>\n",
    "\n",
    "**is-a** relations + various **modifier** relations\n",
    "\n",
    "<center>\n",
    "<img src=\"img/ontology-example.png\" alt=\"drawing\" width=\"700\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/title-graphics.png\" alt=\"drawing\" width=100/>\n",
    "\n",
    "<center> <h1> Challenges of Learning in Taxonomy Spaces </h1> </center>\n",
    "\n",
    "The more detailed is the ontology, the sparser is the decision space\n",
    "\n",
    "\n",
    "<table style=\"\">\n",
    " <tr>\n",
    "    <td style=\"text-align:center\"><b style=\"font-size:30px\"> simple \"flat\" labels</b></td>\n",
    "    <td><b style=\"font-size:30px\">    </b></td>\n",
    "    <td style=\"text-align:center\"><b style=\"font-size:30px\"> taxonomy</b></td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td style=\"text-align:center\">dogs vs cats</td>\n",
    "    <td><br/></td>\n",
    "    <td style=\"text-align:center\">dog and cat breeds</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td><img src=\"img/dog-vs-cat.png\" alt=\"drawing\" width=\"600\"/>\n",
    " </td>\n",
    "    <td></td>\n",
    "    <td> <img src=\"img/dog-cat-trees.png\" alt=\"drawing\" width=\"800\"/> </td>\n",
    " </tr>\n",
    "  <tr>\n",
    "    <td><a href=\"https://www.petbacker.com/\">source </a></td>\n",
    "    <td></td>\n",
    "    <td> <a href=\"https://www.creative-biostructure.com/\">source </a>, \n",
    "        <a href=\"https://doi.org/10.1016/j.ygeno.2007.10.009\">source </a> </td>\n",
    " </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1> Challenges of Evaluation of Classification Results in Taxonomy Spaces </h1> </center>\n",
    "\n",
    "<center> <h3> The more detailed is the taxonomy, the sparser is the decision space </h3> </center>\n",
    "\n",
    "### simple flat labels: accuracy = 100%\n",
    "\n",
    "label | prediction\n",
    "-- | --\n",
    "dog | dog\n",
    "cat | cat\n",
    "dog | dog\n",
    "dog | dog\n",
    "cat | cat\n",
    "\n",
    "\n",
    "### taxonomy: accuracy = 0%\n",
    "\n",
    "label | prediction\n",
    "-- | --\n",
    "Corgi | German Shepard\n",
    "Siamese | Ragdoll\n",
    "Rottweiler | Dobermann\n",
    "Pointer | Dalmatian\n",
    "Persian | Munchkin\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center> <h1> Outstanding Questions </h1> </center>\n",
    "\n",
    "## How to compare two sets of data labels under a taxonomy?\n",
    "  - e.g. is `Corgi` $\\approx$  `German Shepard`?\n",
    "  - e.g. is diagnosis of `breast cancer` $\\approx$ `invasive ductal carcinoma` ?\n",
    "  - what is a good formal metric? aka \"the question is: what is the question?\"\n",
    "  \n",
    "\n",
    "## How to train machine learning models that are aware of taxonomy semantic space? \n",
    "   - i.e. meaningfully group similar concepts and separate unrelated ones\n",
    "  \n",
    "## What method of embedding is the best for training downstream predictive models?\n",
    "Embeddings have been used as input for downstream applications: e.g. Word2Vec and GloVe have been used to embed text for LSTM models or transformers. \n",
    "- Do ontology-aware embeddings perform better in downstream tasks? \n",
    "- Among them, which ones perform best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center> <h1> Schools of Thought </h1> </center>\n",
    "\n",
    "- **Bayesian network approach** approach: model the graph nodes explicitly\n",
    " - how to efficiently learn, predict, and score on a graph?\n",
    " - how to improve computational efficiency compared to \"flat\" labels? [review](https://ruder.io/word-embeddings-softmax/index.html)\n",
    " \n",
    " \n",
    "- **Representation / Embedding / Metric** approach: embed the hierarchy into a metric space\n",
    " - shift reasoning about ontologies\n",
    "   - from the graph space (is X a parent of Y) \n",
    "   - to a metric space (\"is X close to Y?\" and \"is X closer than Y to the coordinate origin?\")\n",
    " - what is a good metric? \n",
    "   - metric learning\n",
    "   - construction of metrics that suit the task\n",
    " - what metrics (and associated manifolds) represent the graph structure most efficiently\n",
    " \n",
    " \n",
    "- [**Spectral graph theory approach**](https://en.wikipedia.org/wiki/Spectral_graph_theory)\n",
    " - leverage spectral properties of [graph adjacency matrix](https://en.wikipedia.org/wiki/Adjacency_matrix) to derive meaningful embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center> <h1> FAQ: Which method should I use? </h1> </center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<center> <h2> (answers may be revealed in the end) </h2> </center>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "<center> \n",
    "But before we get there: What you are trying to accomplish?\n",
    "</center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center> <h1> Bayesian Network Approach: Papers </h1> </center>\n",
    "<center> <img src=\"img/title-graphics.png\" alt=\"drawing\" width=100/></center> \n",
    "\n",
    "## Hierarchical Probabilistic Neural Network Language Model\n",
    "\n",
    "- Morin and Bengio, 2005, [pdf](https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf)\n",
    "- Focus on **speed-up** (compared to \"flat\" labels) using Information Theory based optimization\n",
    "- **not concerned** with representation of **taxonomies per se**: \n",
    "  - a dead horse in all subsequent taxonomy papers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center> <h1> Bayesian Network Approach: Papers </h1> </center>\n",
    "<center> <img src=\"img/title-graphics.png\" alt=\"drawing\" width=100/></center> \n",
    "\n",
    "## Using Ontologies To Improve Performance In Massively Multi-label Prediction \n",
    "- Ethan Steinberg, Peter J. Liu, 2018\n",
    "(Google research)\n",
    "- Datasets: ICD-9 taxonomy and Gene Ontology\n",
    "- Tasks:  \n",
    " - (1) predicting diseases in the form of ICD-9 codes from 2-years of electronic medical records (EMR) from 16M patients\n",
    " - (2) predict protein functions in the form of Gene Ontology (GO) terms from sequence data\n",
    "- Result: TLDR: *helpful given class imbalance*\n",
    "> the Bayesian network of sigmoid output layer has better AUROC and average precision **for rare labels** in all three tasks, with the **effect diminishing with increasing numbers of positive labels**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center> <h1> Bayesian Network Approach: Papers (continued) </h1> </center>\n",
    "<center> <img src=\"img/title-graphics.png\" alt=\"drawing\" width=100/></center> \n",
    "\n",
    "## NBDT: Neural-Backed Decision Trees\n",
    "- Alvin Wan, Dunlap, Ho, Yin, Lee, Jin, Petryk, Bargal, Joseph E. Gonzalez, 2020\n",
    "(UC Berkeley)\n",
    "[pdf](http://nbdt.alvinwan.com/paper/) \n",
    "[github](https://github.com/alvinwan/neural-backed-decision-trees)\n",
    "[home](https://research.alvinwan.com/neural-backed-decision-trees/)\n",
    "\n",
    "- Datasets:  CIFAR100, CIFAR10, ImageNet, TinyImageNet \n",
    "- Task: classification of images (computer vision)\n",
    "\n",
    "> jointly improve both (1) **accuracy** and (2) **interpretability** of modern neural networks, utilizing decision rules that preserve (3) properties like sequential, discrete decisions; pure leaves; and non-ensembled predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center> <h1> Neural-Backed Decision Trees: Hierarchy construction</h1> </center>\n",
    "\n",
    "**Motivation**: Current decision-tree-based methods are either **too loose or  too strict**\n",
    "\n",
    "- (a) hierarchies built with **data-dependent heuristics** like information gain, including *hierarchical softmax* -- **overfit** to the data \n",
    "- (b) **existing hierarchies** like WordNet -- focuses on **conceptual rather than visual similarity**\n",
    "   - according to WordNet: $d(Cat, Bird) < d(Airplane, Bird)$\n",
    "   - visually:  $d( Cat, Bird) > d(Airplane, Bird)$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center> <h1> NBDT: Hierarchy construction</h1> </center>\n",
    "<center> <h2> by hierarchical agglomerative clustering of class-member features</h2> </center>\n",
    "\n",
    "<center>\n",
    "<img src=\"img/wan-2020-fig2.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "build a hierarchy using model weights: run **hierarchical agglomerative clustering** on the normalized class representatives from the feature vectors of the fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1> Neural-Backed Decision Trees</h1> </center>\n",
    "<center> <h2> Inference on Trees</h2> </center>\n",
    "\n",
    "<center>\n",
    "<img src=\"img/wan-2020-fig1c.png\" alt=\"drawing\" height=\"300\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1> Neural-Backed Decision Trees</h1> </center>\n",
    "<center> <h2> Results</h2> </center>\n",
    "\n",
    "<center>\n",
    "<img src=\"img/wan-2020-table6.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1> Neural-Backed Decision Trees</h1> </center>\n",
    "<center> <h1 style=\"color:blue\"> Take-aways </h1> </center>\n",
    "\n",
    "- **accuracy** and **interpretability** can be jointly improved.\n",
    "\n",
    "- **Tree Supervision Loss**, a hierarchical classifier technique that improves over hierarchical softmax\n",
    "\n",
    "- the **Induced Hierarchy**, built from the weights of a pretrained neural network, outperforms existing taxonomies (e.g., WordNet) and data-based hierarchies (e.g., using information gain), demonstrating a hierarchy built-in weight space is most effective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"img/title-graphics.png\" alt=\"drawing\" width=80/></center> \n",
    "<center> <h1> Metric Representationist Approach </h1> </center>\n",
    "\n",
    "### Self-alignment pre-training for biomedical entity representations\n",
    "- Fangyu Liu, ... Nigel Collier, 2020 (University of Cambridge & UCL)\n",
    "- ❓ How to train broad-purpose machine learning models that are aware of taxonomy semantic space? \n",
    " \n",
    "### Poincaré embedding for learning hierarchical representations\n",
    "- Nickel and Kiela, 2017 (Facebook Research)\n",
    "- ❓How to compare two sets of data labels under a taxonomy?\n",
    " \n",
    "### HyperE: Hyperbolic Embeddings for Entities\n",
    "\n",
    "- Gunel, Sala, Gu, Ré, 2018 (Stanford University and Cornell University)\n",
    "- ❓How to compare two sets of data labels under a taxonomy in an embedding space?\n",
    "- ❓How to efficiently and precisely embed a taxonomy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1> Metric Representationist Approach (continued) </h1> </center>\n",
    "\n",
    "### Learning Electronic Health Records through Hyperbolic Embedding of Medical Ontologies\n",
    "- Lu, et al., 2019 (University of Oregon, Eugene & IBM Research)\n",
    "- ❓How to compare two sets of data labels under a taxonomy in an embedding space?\n",
    "- ❓What way of embedding is the best for training downstream predictive models? \n",
    "\n",
    "### Using hyperbolic large-margin classifiers for biological link prediction\n",
    "- Agibetov, Dorffner, Samwald, SemDeep@IJCAI 2019\n",
    "(Medical University of Vienna)\n",
    "[pdf](https://www.aclweb.org/anthology/W19-5805.pdf)\n",
    "- ❓What way of embedding is the best for training downstream predictive models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"img/title-graphics.png\" alt=\"drawing\" width=100/></center> \n",
    "<img src=\"img/liu-title.png\" alt=\"drawing\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definitions\n",
    "\n",
    "### medical entity linking (MEL) task\n",
    "\n",
    "aims to map a medical mention to a well-defined controlled vocabulary (usually a knowledge graph). \n",
    "\n",
    "#### heterogeneous naming issue\n",
    "\n",
    "> For example, the medication _Hydroxychloroquine_ (CUI: C0020336) is often referred to as\n",
    "> - _Oxichlorochine_ (alternative name), \n",
    "> - _HCQ_ (social media) \n",
    "> - _Plaquenil_ (brand name). \n",
    "\n",
    "> **Medical entity linking** is a segue to tackle this problem by framing it as a task of mapping entity mentions to unified concepts in a medical knowledge graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Definitions (continued)\n",
    "\n",
    "### supervised learning\n",
    "- given pairs of examples: $(X_i, y_i)$ aka training set\n",
    "  - $X$ aka input / features,\n",
    "  - $y$ aka ground truth [output] / labels\n",
    "- learn a **mapping** from input to prediction $F: X \\rightarrow \\hat{y}$\n",
    "  - mapping aka function / predictor / predictive model\n",
    "  - prediction aka $\\hat{y}$\n",
    "- so that the **distance** between the prediction and the output  $D\\big(y, \\hat{y} \\big)$ is small (as per **metric / distance function** $D$)\n",
    "- practically, it is accomplished by minimizing a **loss function** $L\\big(y, \\hat{y} \\big)$, which must be continuous. It can be either the distance function itself, or some function, minimization of which will lead to minimization of the distance between the prediction and the ground truth. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Definitions (continued)\n",
    "\n",
    "### self-supervised learning\n",
    " - A form of unsupervised learning where the data provides the supervision ([source: Andrew Zisserman](https://project.inria.fr/paiss/files/2018/07/zisserman-self-supervised.pdf))\n",
    "  - In general, withhold some part of the data, and task the model with predicting it \n",
    "  ([source: Andrew Zisserman](https://project.inria.fr/paiss/files/2018/07/zisserman-self-supervised.pdf))\n",
    "  - e.g. masked language learning: predict a word from its context\n",
    "  \n",
    "    `Roses are red.`   $\\rightarrow$ ($X=$ `__ are red.`, $y=$ `Roses`)\n",
    "    \n",
    "    `The Missippie River flows into the Gulf of Mexico` $\\rightarrow$ (`The Missippie River flows into the __ of Mexico`, `Gulf`)\n",
    "    \n",
    "     - [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) ([Mikolov, 2013](https://arxiv.org/abs/1301.3781)): a simple method of learning word embeddings\n",
    "     - [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) ([Devlin et al, 2018](https://arxiv.org/abs/1810.04805v2)): a [transformer language model](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) that can be used for a wide range of NLP tasks\n",
    "     \n",
    "   - e.g. inpainting task in Computer Vision\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definitions (continued)\n",
    "\n",
    "### self-supervised learning\n",
    " - A form of unsupervised learning where the data provides the supervision ([source: Andrew Zisserman](https://project.inria.fr/paiss/files/2018/07/zisserman-self-supervised.pdf))\n",
    "  - In general, withhold some part of the data, and task the model with predicting it \n",
    "  ([source: Andrew Zisserman](https://project.inria.fr/paiss/files/2018/07/zisserman-self-supervised.pdf))\n",
    "  - e.g. masked language learning: predict a word from its context\n",
    "\n",
    "<br/> \n",
    "    \n",
    "   `Roses are red.`   $\\rightarrow$ ($X=$ `__ are red.`, $y=$ `Roses`)\n",
    "\n",
    "<br/>\n",
    "\n",
    "- Examples:\n",
    " - [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) ([Mikolov, 2013](https://arxiv.org/abs/1301.3781)): a simple method of learning word embeddings\n",
    " - [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) ([Devlin et al, 2018](https://arxiv.org/abs/1810.04805v2)): a [transformer language model](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) that can be used for a wide range of NLP tasks\n",
    " - inpainting task in Computer Vision\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definitions (continued)\n",
    "\n",
    "### self-supervised learning\n",
    "   - e.g. masked language learning: predict a word from its context\n",
    "   - e.g. inpainting task in Computer Vision\n",
    "   \n",
    "<center>\n",
    "<img src=\"img/pathak2016.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "\n",
    "[Pathak et al, 2016](https://arxiv.org/pdf/1604.07379.pdf)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definitions: metric learning \n",
    "\n",
    "  - sometimes, when it is not clear what is the right distance metric to use,\n",
    "  - there is still some knowledge about similarity of inputs:\n",
    "     - one can tell whether two inputs are similar / close together (+1) or dissimilar / far apart (-1):  \n",
    "      diad loss, as e.g. in siamese networks\n",
    "     - one can tell whether one pair is more similar (closer) than the other pair: triplet loss\n",
    "      triplet loss and variations\n",
    " \n",
    "<center>\n",
    "<img src=\"img/triplet-training.png\" alt=\"drawing\" width=\"300\"/>\n",
    "   \n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Definitions : metric learning \n",
    "#### parametric learning\n",
    "\n",
    "Assumes a Mahalanobis (a modified Eucledian) or some other distance metric\n",
    "\n",
    "#### non-parametric metric learning\n",
    "Does not assume any distance metric\n",
    "  - e.g. Multi-Similarity loss:     \n",
    "      a triplet loss (in a broader sense): requires at least 3 examples in a batch for the loss to produce meaningful results \n",
    "$$\\mathcal{L} = \\frac{1}{m} \\sum_{i=1}^m \\left( \n",
    "\\frac1\\alpha \\log \\left[1 + \\sum_{k \\in \\mathcal{P_i} } e^{-\\alpha (S_{ik}-\\lambda)}\\right] + \n",
    "\\frac1\\beta \\log \\left[1 + \\sum_{k \\in \\mathcal{N_i} } e^{+\\beta (S_{ik}-\\lambda)}\\right] \\right) $$\n",
    "   Principle:\n",
    "   - push similar / same-class vectors ($S_{ik}, k \\in \\mathcal{P_i}$) closer together, \n",
    "   - push dissimilar / diffrent-class vectors ($S_{ik}, k \\in \\mathcal{N_i}$) further apart\n",
    "  - see also: AUC loss ([Zhao et al, 2011](https://icml.cc/2011/papers/198_icmlpaper.pdf), [Sulam et al., 2017](https://drive.google.com/file/d/0B3-4p5Bx0j11ZlE4cUVSbExBalE/view)) models Mann-Whittney test loss, which is related to the receiver-operator characteristic AUC (ROC-AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Definitions: non-parametric metric learning\n",
    "\n",
    "<center>\n",
    "<img src=\"img/triplet-training.png\" alt=\"drawing\" width=\"300\"/>\n",
    "   \n",
    "</center>\n",
    "\n",
    "#### Self-Alignment:\n",
    "  \n",
    "  > Given any pair of tuples $(x_i, y_i),(x_j, y_j) ∈ X × Y$, the goal of the **self-alignment** is to learn a function $f (·; \\theta) : X → \\mathbb{R}^d$ parameterised by $\\theta$. Then, the similarity $⟨f(x_i),f(x_j)⟩$ (e.g. cosine similarity) can be used to estimate the resemblance of $x_i$ and $x_j$ (i.e., high if $x_i , x_j$ are synonyms and low otherwise).\n",
    "    \n",
    "  i.e. the final criterion (cosine similarity, which pertains to L2 distance) is a \"parametric\" distance. However, it is not used in the loss function. Instead, it is evaluated after the optimization\n",
    "  \n",
    "  thus the **self-alignment** is formulated *parametrically*, and it is an _a posteriori_ score used to evaluate results of the non-parametric Multi-Similarity Loss training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Approach\n",
    "\n",
    "> We pre-train BERTs on UMLS with a self-alignment objective that clusters synonyms of the same concept using a metric learning loss.\n",
    "\n",
    "> We show that with a well-crafted non-parametric metric learning formulation which scales well on UMLS, an end-to-end Transformer-based language model is sufficient to perform well on the task of medical entity linking (MEL).\n",
    "\n",
    "### Sampling\n",
    "> **Online Hard Pairs Mining**: The online sample mining finds hard positive/negative pairs or triplets within a mini-batch for efficient training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-Alignment\n",
    "\n",
    "\n",
    "<img src=\"img/liu-fig1.png\" alt=\"drawing\" height=\"400\"/>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1> Performance Evaluation </h1> </center>\n",
    "\n",
    "<center>\n",
    "<img src=\"img/liu-table2.png\" alt=\"drawing\" height=\"400\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1 style=\"color:blue\"> Take-aways </h1> </center>\n",
    "\n",
    "- meaningful embeddings are not guaranteed to arise in transformer models (such as BERT) with a general-purpose masked learning\n",
    "- meaningful representations in a Euclidian space can be learned using a non-parametric loss in conjunction with taxonomy data\n",
    "\n",
    "- there are several UMLS-labelled corpora available for benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"img/title-graphics.png\" alt=\"drawing\" width=100/></center> \n",
    "<img src=\"img/nickel-title.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1> Background </h1> </center>\n",
    "\n",
    "- Standard language models (Word2vec, Glove) use Euclidian $\\mathbb{R}^n$ embeddings\n",
    "\n",
    "> Although [Euclidian] embedding methods have proven successful in numerous applications, they suffer from a fundamental limitation: their **ability to model complex patterns is inherently bounded by the dimensionality of the embedding space**\n",
    "\n",
    "\n",
    "- **Hyperbolic geometry** can solve this problem for datasets with (latent) hierarchy, as it _models tree structure_\n",
    "\n",
    ">Since the ability to express information is a precondition for learning and generalization, it is therefore important to [efficiently] increase the **representation capacity** of embedding methods such that they can realistically be used to model complex patterns on a large scale\n",
    "\n",
    "\n",
    "  - the hyperbolic disc area and circle length grow exponentially with their radius\n",
    "  - the number of children grows exponentially with their distance to the root of the tree ~ $b^k$, $2^k$ for binary trees\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/2560px-Comparison_of_geometries.png\" alt=\"drawing\" height=\"400\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"img/geodesics.png\" alt=\"drawing\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Hyperbolic surface example\n",
    "<img src=\"img/kale.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- **Hyperbolic geometry** can solve this problem for datasets with (latent) hierarchy, as it _models tree structure_\n",
    "\n",
    "\n",
    "  - the hyperbolic disc area and circle length grow exponentially with their radius\n",
    "  - the number of children grows exponentially with their distance to the root of the tree ~ $b^k$, $2^k$ for binary trees\n",
    "\n",
    " - distance between two points is defined\n",
    "$$d(\\mathbf{u}, \\mathbf{v}) = \\mathrm{arcosh} \\left(1 + 2 \\frac{||\\mathbf{u} − \\mathbf{v}||^2}{(1 − ||\\mathbf{u}||^2)(1 − ||\\mathbf{v}||^2)} \\right)$$\n",
    "   - circumference of a circle is $2\\pi R \\sinh \\left( \\frac{r}{R} \\right)$\n",
    "   - area of a disk is $4\\pi R^2 \\sinh^2 \\frac{r}{2R} = 2\\pi R^2 \\left(\\cosh \\frac{r}{R} - 1\\right)$\n",
    " \n",
    "- compare to the usual Euclidean $L_2$ distance : $||\\mathbf{u} − \\mathbf{v}||^2$\n",
    "   - circumference of a circle $2 \\pi r$\n",
    "   - area of a disk $\\pi r^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"img/nickel-2017-fig-1b.png\" alt=\"drawing\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training set\n",
    "\n",
    "**WordNet®** is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations\n",
    "\n",
    "## sampling\n",
    "- For training, we randomly sample 10 negative examples per positive example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1> Performance Evaluation </h1> </center>\n",
    "\n",
    "- tasks\n",
    " - reconstruction: `tree --> embedding --> tree`\n",
    " -  prediction of transitive closure relations:\n",
    "  `(A is B) & (B is C) => A is C`\n",
    "  \n",
    "- MAP: mean Average Precision\n",
    "\n",
    "<img src=\"img/nickel-2017-table1.png\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "**Take-away**: Hyperobolic space represents `is-a` relations between concepts **more efficiently** than Euclidean space per dimension added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A Poincaré embedding with $d = 5$. Ground-truth `is-a` relations of the original WORDNET tree are indicated via blue edges. \n",
    "\n",
    "\n",
    "<img src=\"img/nickel-2017-fig-2b.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1 style=\"color:blue\"> Take-aways </h1> </center>\n",
    "\n",
    "- Hyperbolic disk (or n-ball) can represent the hierarchical taxonomies / ontologies more efficiently than Euclidean space of the same dimensionality\n",
    "- Distance in hyperbolic embedding space faithfully and efficiently represents `is-a` relations between taxonomy terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/title-graphics.png\" alt=\"drawing\" width=100/>\n",
    "\n",
    "<center> <h1> HyperE: Hyperbolic Embeddings for Entities</h1> </center>\n",
    "\n",
    "<center>Beliz Gunel, Fred Sala, Albert Gu, Christopher Ré, 2018</center>\n",
    "<center>Stanford University and Cornell University</center>\n",
    "\n",
    "\n",
    "[home](https://hazyresearch.stanford.edu/hyperE/)\n",
    "[arxiv](https://arxiv.org/pdf/1804.03329.pdf)\n",
    "[github](https://github.com/HazyResearch/hyperbolics)\n",
    "\n",
    "\n",
    "<center> <h2 style=\"color:blue\"> Take-aways </h2> </center>\n",
    "\n",
    "- Compare optimization algorithms and **optimization-free embedding construction** techniques\n",
    "- Propose a method of construction and optimization that has lower distortion than the FB Research method\n",
    "- Provide pre-trained UMLS ICD hyperbolic embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"img/lu-2019-title.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "[pdf](https://www.cs.uoregon.edu/Reports/DRP-202003-Lu.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Results\n",
    "###  Comparison similarities expressed as embedding space distance* with graph distance (ontology-based similarities)\n",
    "\n",
    "*including Poincaré hyperbolic distance\n",
    "\n",
    "> We compute the **distance-based term pair similarities in\n",
    "the embedding spaces**, and we evaluate the embeddings by comparing the Pearson Correlation Coefficients between the sequences of\n",
    "**distance-based term pair similarities** and the sequences of **ontology-based term pair similarities**\n",
    "\n",
    "> the Poincaré embeddings significantly outperform the TransE, DistMult, ComplEx and Rescal embeddings, in that **the Poincaré embeddings show much higher correlation coefficients with the ontology-based similarity sequences**. Generally it shows that the similarities between concepts are better retained in the hyperbolic embedding space than in the other embedding spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Reference: Similarity measures\n",
    "\n",
    "- Wu & Palmer similarity\n",
    "  $$Sim_{WUP}(C_1,C_2) = \\frac{2 \\cdot N_3}{N_1 + N_2 + 2 \\cdot N_3}$$\n",
    "  where $N_1 = d(C_1, LCS)$, $N_2 = d(C_1, LCS)$  and $N_3  = d(LCS,0)$\n",
    "- Leacock & Chodorow similarity\n",
    "  $$Sim_{LCH}(C_1,C_2) = - \\log \\frac{ \\mathrm{ShortestPath}(C_1, C_2)}{2 \\cdot  \\mathrm{depthmax}} $$\n",
    "- RADA similarity\n",
    "  $$Sim_{RADA}(C_1,C_2) = 2 \\cdot depthmax − \\mathrm{ShortestPath}(C_1,C_2)$$\n",
    "- Resnik similarity\n",
    "  $$Sim_{RESNIK}(C_1,C_2) = \\mathrm{InformationContent}(\\mathrm{LCS}(C_1,C_2))$$\n",
    "  $$IC(c) = 1 − \\frac{\\log(n_{hyponyms}(c) + 1)}{\\log(maxnodes)}$$\n",
    "\n",
    "**Definitions:** least common subsumer (LCS) $\\approx$ latest common ancestor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results: Correlation of embedding- and graph-distance metrics\n",
    "\n",
    "<img src=\"img/lu-2019-table-1.png\" alt=\"drawing\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Results: Prediction on MIMIC-III \n",
    "\n",
    "### 30-day Unplanned ICU Readmission Prediction\n",
    "> the Poincaré embeddings outperform all other graph embedding methods **except TransE**\n",
    "\n",
    "<img src=\"img/lu-2019-table-3.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Results: Prediction on MIMIC-III \n",
    "\n",
    "### in In-Hospital Mortality Prediction\n",
    "> The results prove the effectiveness of our method by representing higher AUROC than the benchmark, though in this task **the hyperbolic embeddings do not outperform all other embeddings**\n",
    "\n",
    "<img src=\"img/lu-2019-table-5.png\" alt=\"drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center> <h1 style=\"color:blue\"> Take-aways </h1> </center>\n",
    "\n",
    "- Hyperbolic embeddings have nice properties, i.e. they compare well to graph-distance\n",
    "\n",
    "- Feeding hyperbolic embeddings into networks that have little awareness of hyperbolic geometry is not guaranteed to produce performanc gains in predictive modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/title-graphics.png\" alt=\"drawing\" width=100/>\n",
    "\n",
    "# Using hyperbolic large-margin classifiers for biological link prediction.\n",
    "\n",
    "Asan Agibetov, Georg Dorffner, Matthias Samwald (2019) SemDeep@IJCAI 2019 [pdf](https://www.aclweb.org/anthology/W19-5805.pdf)\n",
    "\n",
    "<center> <h2 style=\"color:blue\"> Take-aways </h2> </center>\n",
    "\n",
    "- Hyperbolic embeddings perform comparably with Euclidean embeddings of the same dimension (in hyperbolic SVM task and classical SVM respectively) in link prediction task on UMLS and BIO-KG datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/title-graphics.png\" alt=\"drawing\" width=100/>\n",
    "\n",
    "# Conclusions\n",
    "\n",
    "- Graph-based methods are good as an output layer in ML models when ontology is known\n",
    "- Hyperbolic embeddings can be useful for scoring ML predictions\n",
    "- Hyperbolic embeddings are comparable in performance with Euclidian embeddings in downstream applications\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
